# -*- coding: utf-8 -*-
"""T5_VariableNamining(Final).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KkXR_oZ7aODFyEuMfyPtjMsRDtyblhK4
"""

!pip install pandas numpy tensorflow transformers

!pip install sentencepiece

import pandas as pd
import numpy as np
import tensorflow as tf
import sentencepiece
from transformers import T5ForConditionalGeneration, T5Tokenizer

# Load the Drive helper and mount
from google.colab import drive
drive.mount('/content/drive')

pre_train_path = '/content/drive/MyDrive/SWE628-Project/Datasets/Pre-Training_DB (Sample).xlsx'

pre_train_db = pd.read_excel(pre_train_path)
print(pre_train_db.shape)

df = pre_train_db.convert_dtypes()
# df = pre_train_db.convert_dtypes()
# df = pd.read_csv('variable_names_data.csv')
df['extractedVars'] = df['extractedVars'].str.strip('[]').str.replace("'","").str.replace(",","").str.split(' ')
method_bodies = df['methodBody'].tolist()
extracted_vars = df['extractedVars'].tolist()

print(type(method_bodies))
print(df.shape)

# Initialize the tokenizer
tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-small')

# Define the function to tokenize your data
def preprocess_function(examples):
    return tokenizer(examples['methodBody'], truncation=True, padding='max_length')

# Apply the preprocess function to the 'methodBody' column
tokenized_dataset = df.apply(lambda row: preprocess_function(row), axis=1)

# Task Configuration
task = "suggest variable name"
input_ids = tokenizer(task, return_tensors="pt").input_ids

# Meta-GPT Model Configuration
model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-small')
# Rationale: Meta-GPT is a powerful language model capable of generating text. It's suitable for this task as it can learn to suggest variable names based on the context of the function body.

import torch
from transformers import T5ForConditionalGeneration, T5Tokenizer
from torch.utils.data import DataLoader, Dataset

# Initialize the tokenizer
tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-small')

# Assuming df is your dataframe and 'methodBody' is the column with text data
unsupervised_dataset = df[df['methodBody'].notna()]
unsupervised_data = unsupervised_dataset['methodBody'].tolist()
tokenized_unsupervised_data = tokenizer(
    unsupervised_data,
    truncation=True,
    padding='max_length',
    return_tensors="pt"  # Important: return PyTorch tensors
)

# Assuming these are the appropriate inputs and labels for model training
input_ids = tokenized_unsupervised_data['input_ids']
attention_mask = tokenized_unsupervised_data['attention_mask']
labels = input_ids.clone()  # Or however you wish to construct your labels

# Custom dataset class for PyTorch
class T5Dataset(Dataset):
    def __init__(self, input_ids, attention_mask, labels):
        self.input_ids = input_ids
        self.attention_mask = attention_mask
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            'input_ids': self.input_ids[idx],
            'attention_mask': self.attention_mask[idx],
            'labels': self.labels[idx]
        }

# Create the dataset and dataloader
dataset = T5Dataset(input_ids, attention_mask, labels)
dataloader = DataLoader(dataset, batch_size=16)

# Unsupervised Pre-Training
print("Unsupervised Pre-Training...")
model_for_pretraining = T5ForConditionalGeneration.from_pretrained('google/flan-t5-small')

# Use a PyTorch optimizer and loss function
optimizer = torch.optim.Adam(model_for_pretraining.parameters(), lr=1e-4)

# Train the model on the unsupervised dataset
model_for_pretraining.train()
for epoch in range(3):  # Number of epochs
    for batch in dataloader:
        optimizer.zero_grad()
        outputs = model_for_pretraining(
            input_ids=batch['input_ids'],
            attention_mask=batch['attention_mask'],
            labels=batch['labels']
        )
        loss = outputs.loss
        loss.backward()
        optimizer.step()
    print(f"Epoch {epoch} completed")

# Save the pre-trained model weights
torch.save(model_for_pretraining.state_dict(), 't5-small-pre-trained-weights.pt')

from sklearn.model_selection import train_test_split

# Assuming 'input_ids' and 'attention_mask' are the features and 'labels' are the target you want to predict.
# You need to split these into train and test sets.

# Split the dataset into training and testing sets
train_features, test_features, train_labels, test_labels = train_test_split(
    tokenized_unsupervised_data['input_ids'],
    labels,  # Assuming you have a separate labels variable prepared
    test_size=0.2,
    shuffle=True
)

# Assuming you have a way to convert these into a format compatible with the T5 model training
# For example, you might have a code snippet that wraps the training data into a TensorFlow dataset or a PyTorch DataLoader.

# Now, you would use `train_features`, `train_labels` for training, and `test_features`, `test_labels` for validation.

from torch.utils.data import DataLoader, random_split

# Assuming dataset is an instance of T5Dataset class containing the preprocessed dataset
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=16)

# Train the model on the supervised dataset
model_for_pretraining.train()
optimizer = torch.optim.Adam(model_for_pretraining.parameters(), lr=1e-4)
for epoch in range(5):  # Change the number of epochs if needed
    # Training loop
    for batch in train_dataloader:
        optimizer.zero_grad()
        outputs = model_for_pretraining(input_ids=batch['input_ids'],
                                        attention_mask=batch['attention_mask'],
                                        labels=batch['labels'])
        loss = outputs.loss
        loss.backward()
        optimizer.step()
    print(f"Epoch {epoch} completed")

    # Validation loop
    model_for_pretraining.eval()
    with torch.no_grad():
        total_loss = 0
        for batch in val_dataloader:
            outputs = model_for_pretraining(input_ids=batch['input_ids'],
                                            attention_mask=batch['attention_mask'],
                                            labels=batch['labels'])
            loss = outputs.loss
            total_loss += loss.item()
    print(f"Validation loss: {total_loss / len(val_dataloader)}")

from sklearn.model_selection import train_test_split

# Verify the sizes of the tensors
print(f'Input IDs size: {input_ids.size()}')
print(f'Attention Mask size: {attention_mask.size()}')
print(f'Labels size: {labels.size()}')

# This should be the same for all tensors, assuming each sample includes all components
assert input_ids.size(0) == attention_mask.size(0) == labels.size(0), "Mismatched sizes!"

# Create the dataset and dataloader
dataset = T5Dataset(input_ids, attention_mask, labels)

# Verify the length of the dataset corresponds to the first dimension of your tensors
assert len(dataset) == input_ids.size(0), "Dataset length mismatch!"

# DataLoader should handle batch sizes correctly, but just to make sure the batch size is not greater than the dataset size
dataloader = DataLoader(dataset, batch_size=min(16, len(dataset)))

# Then you would write a loop to perform evaluation on your test data.
model.eval()
total_loss = 0

with torch.no_grad():
    for batch in dataloader:
        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'])
        total_loss += outputs.loss.item()

average_loss = total_loss / len(dataloader)
print(f'Test loss: {average_loss}')

