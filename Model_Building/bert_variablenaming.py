# -*- coding: utf-8 -*-
"""BERT_VariableNaming.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tFFN2to3pDiVkaZBzmVib1DHzhrFyGHE
"""

!pip install transformers
!pip install torch

import pandas as pd
import numpy as np
from transformers import BertTokenizer, BertForMaskedLM, GPT2Tokenizer, GPT2LMHeadModel
import torch

# Load the Drive helper and mount
from google.colab import drive
drive.mount('/content/drive')

pre_train_path = '/content/drive/MyDrive/SWE628-Project/Datasets/Pre-Training_DB (Sample).xlsx'

pre_train_db = pd.read_excel(pre_train_path)
print(pre_train_db.shape)

df = pre_train_db.convert_dtypes()
df['extractedVars'] = df['extractedVars'].str.strip('[]').str.replace("'","").str.replace(",","").str.split(' ')
method_bodies = df['methodBody'].values.tolist()
extracted_vars = df['extractedVars'].values.tolist()

print(type(method_bodies))
print(df.shape)

# Preprocess 'methodBody' text data: tokenization and padding.
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
# padded_method_bodies = tokenizer(method_bodies, padding=True, return_tensors='pt').input_ids
# Pad sequences to a consistent length of 512 tokens.
padded_method_bodies = tokenizer(method_bodies, padding='max_length', truncation=True, return_tensors='pt').input_ids

# Create a mapping between variable names in 'extractedVars' and suggested names.
var_name_mapping = {'x': 'input_value', 'y': 'output_value', 'z': 'auxiliary_variable'}

# Model Selection: BERT
# BERT is a pre-trained language model suitable for this task due to its bidirectional context encoding.

# Pre-Training (Unsupervised):
# Masked Language Modeling
masked_method_bodies = method_bodies.copy()
for i in range(len(masked_method_bodies)):
    # Randomly mask 15% of the tokens in each method body.
    indices_to_mask = np.random.choice(len(method_bodies[i]), int(len(method_bodies[i]) * 0.15), replace=False)
    masked_method_bodies[i] = ' '.join(['[MASK]' if j in indices_to_mask else word for j, word in enumerate(method_bodies[i].split())])

# Fine-Tuning:
# Fine-tune BERT on the specific task of variable name suggestion.
model = BertForMaskedLM.from_pretrained('bert-base-uncased')
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)

text = 'this is bold'
print(f'\033[1mEpoch: {text}, -- Ended --\033[0m')

# Train the model on the training set.
model.train()
# batch_size = 15  # Adjust the batch size based on your GPU memory
# batch_size = 128 # Next Try
batch_size = 4 # Next Try
epochs = 5
# epochs = 10 # Next Try

for epoch in range(5):
  print(f'\033[1mEpoch: {epoch+1}/{epochs}, -- Strated --\033[0m')
  loop_range = len(masked_method_bodies)
  for i in range(0, len(masked_method_bodies), batch_size):
    input_ids = tokenizer(masked_method_bodies[i:i+batch_size], padding='max_length', truncation=True, return_tensors='pt').input_ids
    labels = input_ids.clone()
    outputs = model(input_ids, labels=labels)
    loss = outputs[0]
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    print(f'Epoch: {epoch+1}/{epochs}, i: {i+1}/{loop_range}, Loss: {loss}')
  print(f'\033[1mEpoch: {epoch+1}/{epochs}, -- Ended --\033[0m')

# Evaluate the model on the test set.
model.eval()
test_acc = 0
size = 4
test_accurcy = 0
for i in range(len(method_bodies)):
    input_ids = tokenizer(method_bodies[i], padding=True, truncation=True, max_length=512, return_tensors='pt').input_ids
    labels = tokenizer(extracted_vars[i], padding=True, truncation=True, max_length=512, return_tensors='pt').input_ids
    outputs = model(input_ids)
    logits = outputs[0]
    predictions = torch.argmax(logits, dim=-1)

    valid_predictions = predictions.resize_(labels.shape[1])
    valid_labels = labels

    test_acc += (valid_predictions == valid_labels).sum()/valid_labels.numel()
print(f'Test Accuracy: {(test_acc / len(method_bodies)):.4f}')

# Save the trained model for future use.
output_dir = '/content/drive/MyDrive/SWE628-Project/output_models/'
torch.save(model.state_dict(), output_dir + 'bert_variable_renaming_model_small.pt')

# Load the trained BERT model.
model = BertForMaskedLM.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

